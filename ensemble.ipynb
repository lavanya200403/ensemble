{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#1.What is Ensemble Learning in machine learning? Explain the key idea behind it.\n",
        "\n",
        "    ->Ensemble learning in machine learning is a method that combines the predictions of multiple individual models (called \"base learners\") to achieve a single, more accurate, and robust prediction than any single model could on its own. The key idea is to leverage the \"wisdom of crowds\" by creating a diverse group of models"
      ],
      "metadata": {
        "id": "tjQKmL3xy8ot"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2.What is the difference between Bagging and Boosting?\n",
        "\n",
        "    ->Bagging is a learning approach that aids in enhancing the performance, execution, and precision of machine learning algorithms. Boosting is an approach that iteratively modifies the weight of observation based on the last classification."
      ],
      "metadata": {
        "id": "qdkDpR7wy8lH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3.What is bootstrap sampling and what role does it play in Bagging methods like Random Forest?\n",
        "\n",
        "    ->Bootstrap sampling is a resampling technique where a subset of data is randomly drawn from an original dataset with replacement."
      ],
      "metadata": {
        "id": "y0Vx66FAy8id"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4.What are Out-of-Bag (OOB) samples and how is OOB score used to evaluate ensemble models?\n",
        "\n",
        "    -> It is calculated using the samples that are not used in the training of the model, which is called out-of-bag samples."
      ],
      "metadata": {
        "id": "xCQaJ0FKy8fV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "jfnyW2eAy8cd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5.Compare feature importance analysis in a single Decision Tree vs. a Random Forest.\n",
        "\n",
        "    ->Single Decision Tree:\n",
        "    \n",
        "    Impurity-based importance\n",
        "    Sensitivity to individual splits\n",
        "\n",
        "    ->Random Forest:\n",
        "\n",
        "    Ensemble averaging\n",
        "    Permutation importance"
      ],
      "metadata": {
        "id": "8hyRTztdy8ZH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6.Write a Python program to:\n",
        "    ● Load the Breast Cancer dataset using\n",
        "    sklearn.datasets.load_breast_cancer()\n",
        "    ● Train a Random Forest Classifier\n",
        "    ● Print the top 5 most important features based on feature importance scores."
      ],
      "metadata": {
        "id": "t6sTtX5vy8TJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "cancer = load_breast_cancer()\n",
        "X = pd.DataFrame(cancer.data, columns=cancer.feature_names)\n",
        "y = cancer.target\n",
        "\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Fit the classifier to the data\n",
        "rf_classifier.fit(X, y)\n",
        "importances = rf_classifier.feature_importances_\n",
        "feature_names = cancer.feature_names\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'feature': feature_names,\n",
        "    'importance': importances\n",
        "})\n",
        "\n",
        "feature_importance_df = feature_importance_df.sort_values(by='importance', ascending=False)\n",
        "\n",
        "print(\"Top 5 Most Important Features in the Breast Cancer Dataset:\")\n",
        "print(feature_importance_df.head(5))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZIZUT4f06iC",
        "outputId": "bcdbfc64-0ece-459c-adf8-7efc335ca9cb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Most Important Features in the Breast Cancer Dataset:\n",
            "                 feature  importance\n",
            "23            worst area    0.139357\n",
            "27  worst concave points    0.132225\n",
            "7    mean concave points    0.107046\n",
            "20          worst radius    0.082848\n",
            "22       worst perimeter    0.080850\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#7.Write a Python program to:\n",
        "● Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
        "● Evaluate its accuracy and compare with a single Decision Tree"
      ],
      "metadata": {
        "id": "_mcTdNfmy8JT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWFuMGsJy2u8",
        "outputId": "d6a48623-189b-42fa-b0a7-cd4b2d0773c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of a single Decision Tree: 1.0000\n",
            "Accuracy of Bagging Classifier: 1.0000\n",
            "\n",
            "Both classifiers achieved the same accuracy.\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "single_dt_classifier = DecisionTreeClassifier(random_state=42)\n",
        "single_dt_classifier.fit(X_train, y_train)\n",
        "\n",
        "y_pred_single_dt = single_dt_classifier.predict(X_test)\n",
        "\n",
        "accuracy_single_dt = accuracy_score(y_test, y_pred_single_dt)\n",
        "print(f\"Accuracy of a single Decision Tree: {accuracy_single_dt:.4f}\")\n",
        "bagging_classifier = BaggingClassifier(n_estimators=10, random_state=42)\n",
        "bagging_classifier.fit(X_train, y_train)\n",
        "\n",
        "y_pred_bagging = bagging_classifier.predict(X_test)\n",
        "\n",
        "accuracy_bagging = accuracy_score(y_test, y_pred_bagging)\n",
        "print(f\"Accuracy of Bagging Classifier: {accuracy_bagging:.4f}\")\n",
        "if accuracy_bagging > accuracy_single_dt:\n",
        "    print(\"\\nThe Bagging Classifier achieved higher accuracy.\")\n",
        "elif accuracy_bagging < accuracy_single_dt:\n",
        "    print(\"\\nThe single Decision Tree achieved higher accuracy.\")\n",
        "else:\n",
        "    print(\"\\nBoth classifiers achieved the same accuracy.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#8.Write a Python program to:\n",
        "    ● Train a Random Forest Classifier\n",
        "    ● Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
        "    ● Print the best parameters and final accuracy"
      ],
      "metadata": {
        "id": "zXti6khU2JdY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "iris = load_iris()\n",
        "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "y = iris.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "rf_classifier = RandomForestClassifier(random_state=42)\n",
        "param_grid = {'n_estimators': [50, 100, 150], 'max_depth': [None, 5, 10]}\n",
        "\n",
        "grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best parameters found by GridSearchCV:\")\n",
        "print(grid_search.best_params_)\n",
        "\n",
        "best_rf_model = grid_search.best_estimator_\n",
        "y_pred = best_rf_model.predict(X_test)\n",
        "final_accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Final accuracy on the test set: {final_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9k2qMvz2SKH",
        "outputId": "feaca133-e72c-4795-9a2a-383cf8ce67d2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters found by GridSearchCV:\n",
            "{'max_depth': None, 'n_estimators': 100}\n",
            "Final accuracy on the test set: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#9.Write a Python program to:\n",
        "    ● Train a Bagging Regressor and a Random Forest Regressor on the California\n",
        "    Housing dataset\n",
        "    ● Compare their Mean Squared Errors (MSE)"
      ],
      "metadata": {
        "id": "34aszjX_281u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "housing = fetch_california_housing()\n",
        "X = housing.data\n",
        "y = housing.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "bagging_reg = BaggingRegressor(random_state=42)\n",
        "bagging_reg.fit(X_train, y_train)\n",
        "y_pred_bagging = bagging_reg.predict(X_test)\n",
        "mse_bagging = mean_squared_error(y_test, y_pred_bagging)\n",
        "\n",
        "rf_reg = RandomForestRegressor(random_state=42)\n",
        "rf_reg.fit(X_train, y_train)\n",
        "y_pred_rf = rf_reg.predict(X_test)\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "print(f\"Mean Squared Error (Bagging Regressor): {mse_bagging:.4f}\")\n",
        "print(f\"Mean Squared Error (Random Forest Regressor): {mse_rf:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdKxk33e3IbF",
        "outputId": "7b7df6e0-1e96-4d53-be2e-23214bb5a07b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (Bagging Regressor): 0.2824\n",
            "Mean Squared Error (Random Forest Regressor): 0.2554\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#10.You are working as a data scientist at a financial institution to predict loan default. You have access to customer demographic and transaction history data.\n",
        "    You decide to use ensemble techniques to increase model performance.\n",
        "    Explain your step-by-step approach to:\n",
        "    ● Choose between Bagging or Boosting\n",
        "    ● Handle overfitting\n",
        "    ● Select base models\n",
        "    ● Evaluate performance using cross-validation\n",
        "    ● Justify how ensemble learning improves decision-making in this real-world\n",
        "    context.\n"
      ],
      "metadata": {
        "id": "hTClsadq3Zz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "    ->(1) choose between Bagging and Boosting by considering the nature of the data and base models.\n",
        "\n",
        "    (2) Overfitting is handled by techniques like pruning base models, adjusting ensemble parameters, and using regularization within base models.\n",
        "\n",
        "    (3) Base models are selected based on their diversity and ability to model complex relationships in the data.\n",
        "\n",
        "    (4) Performance is evaluated using cross-validation, calculating metrics like precision, recall, and AUC on multiple splits of the data to get a robust performance estimate.\n",
        "    \n",
        "    (5) Decision-making improves because ensembles provide more stable, accurate, and reliable predictions"
      ],
      "metadata": {
        "id": "ICk-BS163rEf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "uCK0gaNh4Nhz"
      }
    }
  ]
}